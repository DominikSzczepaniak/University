# Modele jÄ™zykowe Ä‡wiczenia 2
# 7/8
## ==ğŸŸ¢Zadanie 1== 
Perplexity to metryka ktÃ³ra mÃ³wi jak Åºle model przewiduje kolejne sÅ‚owo. Im niÅ¼szy wynik tym lepszy jest model.
n-gram - przewidujemy kolejne sÅ‚owo na podstawie n-1 poprzednich sÅ‚Ã³w
unigramowy - korzystamy tylko z ostatniego sÅ‚owa
bigramowy - korzystamy tylko z dwÃ³ch ostatnich sÅ‚Ã³w
k = 10

ZakÅ‚adamy, Å¼e tokeny to pojedyczne cyfry. Wtedy n = k, Å¼ebyÅ›my wiedzieli kiedy koÅ„czy siÄ™ blok i robimy ppb jednostajne na dowolnÄ… liczbÄ™. 
Dla unigramu i bigramu nie wiemy w zasadzie nic, wiÄ™c losujemy z takim samym ppb kolejnÄ… cyfrÄ™. Czyli perplexity = 10
Dla n-gramowego uÅ¼ywamy n=10 i wtedy wiemy kiedy moÅ¼emy zakoÅ„czyÄ‡ blok, a kiedy losowaÄ‡ losowÄ… cyfrÄ™ ktÃ³ra zacznie kolejny blok. 
Perplexity ma wzÃ³r:
![](Modele%20je%CC%A8zykowe%20c%CC%81wiczenia%202/Zrzut%20ekranu%202024-11-3%20o%2017.52.06.png)
Gdzie x to wydarzenia, a p(x) to ich prawdopodobieÅ„stwa. Mamy 10 wydarzeÅ„ - 9 z nich to kontynuowanie bloku, a 1 to losowanie kolejnego wydarzenia, wiÄ™c nasze perplexity jest rÃ³wne
![](Modele%20je%CC%A8zykowe%20c%CC%81wiczenia%202/Zrzut%20ekranu%202024-11-3%20o%2017.54.02.png) = 10

## ==ğŸŸ¢Zadanie 2== 
JeÅ›li perplexity jest liczone na podstawie prawdpodobieÅ„stwa kolejnych wyrazÃ³w w zdaniu, to jeÅ¼eli perplexity ogÃ³lnej prÃ³bki statystycznej jest mniejsze niÅ¼ danych tekstÃ³w, to te teksty miaÅ‚y inny rozkÅ‚ad prawdopodobieÅ„stwa niÅ¼ ogÃ³lna prÃ³bka statystyczna tekstÃ³w. PoniewaÅ¼ model statystyczny trzyma bardzo duÅ¼Ä… iloÅ›Ä‡ tekstÃ³w w swojej pamiÄ™ci, to moÅ¼na zaÅ‚oÅ¼yÄ‡, Å¼e jego prÃ³bka statystyczna jest tÄ… poprawnÄ…. To byÅ‚oby rozwiÄ…zanie w sytuacji idealnej, w ktÃ³rej bot produkuje teksty bez wpÅ‚ywu promptera. 

Model jÄ™zykowy moÅ¼e stosowaÄ‡ natomiast rÃ³Å¼ne techniki - np. znak wodny albo zostaÄ‡ zmuszony przez promptera na odpowiadanie w dany sposÃ³b. Wtedy prawdopodobieÅ„stwo bÄ™dzie zmienione i prÃ³bka statystyczna z wszystkich tekstÃ³w nie bÄ™dzie miarodajna. Prompter moÅ¼e prosiÄ‡ o takie odpowiedzi ktÃ³re w normalnym Å›wiecie majÄ… bardzo niskie prawdopodobieÅ„stwo wystÄ…pienia. 

## ==ğŸŸ¢Zadanie 3== 
**a)** JeÅ¼eli na k-tej pozycji mamy okreÅ›lony wyraz, a jest on rzadki np. w jÄ™zyku angielskim zaczynamy zdania od I, ale rzadko kiedy I wystÄ™puje w Å›rodku zdania (oprÃ³cz np. zdaÅ„ pytajÄ…cych) no to jeÅ¼eli nie zaczniemy zdania od formy pytajÄ…cej to moÅ¼e siÄ™ trafiÄ‡, Å¼e jeÅ¼eli wstawimy sÅ‚owo na k-tÄ… pozycjÄ™ to nie bÄ™dzie istnieÄ‡ ppb dla 2 czy 3 gramowego modelu, ani w zasadzie dla Å¼adnego innego. 

Chcemy wybrac takie wyrazy aby P(xk | xk-1 xk-2 xk-3 â€¦) bylo jak najwieksze.
Ja bym najpierw wybral teksty ktore maja najwieksze prawdopodobienstwo P(xk | xk-1 xk-2) w modelach 2 gramowych i odpowiednie ppb w modelu 3-gramowym. Nastepnie na podstawie tego musialbym wybierac tylko jeden wyraz aby uzupelniac poprzednikow. Generowanie tekstow na prawo od k jest Å‚atwe znajÄ…c poprzednie wyrazy.
Czyli generowaÅ‚bym bardzo duÅ¼o tekstÃ³w i sprawdzaÅ‚ ktÃ³re majÄ… na k-tym miejscu wyraz ktÃ³ry ma tam byÄ‡.

Jednym z pomysÅ‚Ã³w byÅ‚oby teÅ¼ branie sÅ‚Ã³w z duÅ¼ym prawdopodobieÅ„stwem i nastÄ™pnie permutacje tych zdaÅ„ w ktÃ³rych dany wyraz ma byÄ‡ na k-tym miejscu.


**b)** Nie sprawdzi siÄ™ poniewaÅ¼ wylosowanie x-1 wyrazÃ³w nie gwarantuje Å¼e wyraz na pozycji x bÄ™dzie pasowaÅ‚. MoÅ¼emy forcowaÄ‡, Å¼e jeÅ¼eli zadnie w ogÃ³le nie bÄ™dzie pasowaÅ‚o to zakoÅ„czymy poprzednie zdanie kropkÄ… i sprÃ³bujemy rozpoczÄ…Ä‡ kolejne. 

**Proponowane rozwiÄ…zanie:**

	1.	**Dynamiczne przypisywanie wartoÅ›ci do pozycji nieparzystych:** Najpierw ustal wartoÅ›ci na pozycjach parzystych, a nastÄ™pnie generuj sÅ‚owa na pozycjach nieparzystych tak, by dopasowaÄ‡ siÄ™ do kontekstu.
	2.	**Metoda wsteczna (backtracking):** JeÅ›li Å¼adne z moÅ¼liwych losowanych sÅ‚Ã³w na pozycji nieparzystej nie pasuje do wyrazu na nastÄ™pnej pozycji parzystej, wracamy do poprzedniej pozycji nieparzystej i zmieniamy wybÃ³r.
	3.	**Metoda sekwencyjnego optymalizowania z uÅ¼yciem beam search:** KaÅ¼da kombinacja zaczyna siÄ™ od pewnych wyrazÃ³w na parzystych pozycjach, a nastÄ™pnie generujemy moÅ¼liwe wartoÅ›ci na nieparzystych pozycjach, co pozwala na dopasowanie kontekstowe.

**c)** Podobnie jak w poprzednim - moÅ¼emy wylosowaÄ‡ takie sÅ‚owa, Å¼e ostatni wyraz bÄ™dzie niemoÅ¼liwy w danym zdaniu. Zdania majÄ… nie byÄ‡ jakoÅ› bardzo dÅ‚ugie, wiÄ™c myÅ›lÄ™, Å¼e beam-search powinien tutaj zadziaÅ‚aÄ‡. Zawsze teÅ¼ jest opcja zakoÅ„czenia jednej czÄ™Å›ci zdania i rozpoczÄ™cia kolejnej po kropce.

**Proponowane rozwiÄ…zanie:**

	1.	**Beam search z optymalizacjÄ… na koÅ„cowe dopasowanie:** Rozpocznij generowanie od pierwszego sÅ‚owa, jednoczeÅ›nie prÃ³bujÄ…c zachowaÄ‡ moÅ¼liwy kontekst koÅ„cowego sÅ‚owa, uÅ¼ywajÄ…c beam search do zachowania kilku najbardziej prawdopodobnych wersji tekstu.
	2.	**Heurystyka ograniczenia dÅ‚ugoÅ›ci:** TworzÄ…c tekst, wprowadÅº heurystykÄ™, aby dÅ‚ugoÅ›Ä‡ byÅ‚a zgodna z zaÅ‚oÅ¼onym M, a rÃ³wnoczeÅ›nie koÅ„czyÅ‚a siÄ™ sÅ‚owem o wysokim prawdopodobieÅ„stwie w kontekÅ›cie ostatniego sÅ‚owa.
	3.	**Dwukierunkowe losowanie:** Rozpocznij generacjÄ™ zarÃ³wno od pierwszego sÅ‚owa w przÃ³d, jak i od ostatniego sÅ‚owa wstecz, Å‚Ä…czÄ…c te fragmenty tam, gdzie kontekst jest spÃ³jny.


**d)** PoniewaÅ¼ takich wyrazÃ³w jest bardzo maÅ‚o i mamy ograniczony wybÃ³r to ppb Å¼e kaÅ¼de kolejne sÅ‚owo bÄ™dzie pasowaÅ‚o do poprzedniego ktÃ³re koÅ„czyÅ‚o siÄ™ na dany sufiks jest bardzo niskie. To co bym zrobiÅ‚ to, ze wzglÄ™du na maÅ‚Ä… iloÅ›Ä‡ wyrazÃ³w, uÅ¼ywaÅ‚ beam-searcha. Rozpatrujemy bardzo duÅ¼o opcji, ale nie jest ich aÅ¼ tak duÅ¼o, a moÅ¼emy siÄ™ cofaÄ‡ do bardziej prawdopodobnych opcji.

Oto bardziej szczegÃ³Å‚owe uzupeÅ‚nienie Twoich odpowiedzi dla kaÅ¼dego z podpunktÃ³w:

### a) Losowanie tekstu o dÅ‚ugoÅ›ci M, ktÃ³ry na pozycji k ma okreÅ›lony wyraz

Problem z naturalnym podejÅ›ciem polega na tym, Å¼e losowe generowanie od poczÄ…tku do koÅ„ca tekstu prowadzi do bardzo niskiej szansy wylosowania sÅ‚owa na pozycji , jeÅ›li to sÅ‚owo ma rzadkie wystÄ…pienie w kontekÅ›cie modelu n-gramowego. Dlatego, po uzyskaniu losowego tekstu, musimy wielokrotnie powtarzaÄ‡ generacjÄ™, co jest nieefektywne.

**Proponowane rozwiÄ…zanie:**

	1.	**Algorytm oparty na maksymalizacji prawdopodobieÅ„stwa kontekstu:** Wybieramy sÅ‚owa do pozycji  i , bazujÄ…c na maksymalnym prawdopodobieÅ„stwie warunkowym, np.  dla bigramÃ³w lub  dla trigramÃ³w. NastÄ™pnie, konstruujemy Å›cieÅ¼ki od poczÄ…tku aÅ¼ do pozycji , by zapewniÄ‡ zgodnoÅ›Ä‡ kontekstowÄ….
	2.	**UÅ¼ycie beam search z priorytetem dla tekstÃ³w o duÅ¼ym prawdopodobieÅ„stwie:** UstalajÄ…c priorytet dla sÅ‚Ã³w najczÄ™Å›ciej pojawiajÄ…cych siÄ™ w kontekÅ›cie z pozycjÄ… , moÅ¼emy generowaÄ‡ kilka rÃ³wnolegÅ‚ych wersji tekstu, a nastÄ™pnie wybraÄ‡ tÄ™, ktÃ³ra speÅ‚nia wymÃ³g wystÄ…pienia sÅ‚owa na pozycji .

### b) Losowanie tekstu o dÅ‚ugoÅ›ci M, ktÃ³ry na pozycjach parzystych ma okreÅ›lone wyrazy

Naturalne losowanie od lewej do prawej czÄ™sto nie speÅ‚ni wymagania, aby wyrazy na pozycjach parzystych odpowiadaÅ‚y z gÃ³ry ustalonym sÅ‚owom, poniewaÅ¼ ich kontekst nie zawsze bÄ™dzie pasowaÅ‚ do tego, co wylosowaliÅ›my na pozycjach nieparzystych.

**Proponowane rozwiÄ…zanie:**

	1.	**Dynamiczne przypisywanie wartoÅ›ci do pozycji nieparzystych:** Najpierw ustal wartoÅ›ci na pozycjach parzystych, a nastÄ™pnie generuj sÅ‚owa na pozycjach nieparzystych tak, by dopasowaÄ‡ siÄ™ do kontekstu.
	2.	**Metoda wsteczna (backtracking):** JeÅ›li Å¼adne z moÅ¼liwych losowanych sÅ‚Ã³w na pozycji nieparzystej nie pasuje do wyrazu na nastÄ™pnej pozycji parzystej, wracamy do poprzedniej pozycji nieparzystej i zmieniamy wybÃ³r.
	3.	**Metoda sekwencyjnego optymalizowania z uÅ¼yciem beam search:** KaÅ¼da kombinacja zaczyna siÄ™ od pewnych wyrazÃ³w na parzystych pozycjach, a nastÄ™pnie generujemy moÅ¼liwe wartoÅ›ci na nieparzystych pozycjach, co pozwala na dopasowanie kontekstowe.

### c) Losowanie niezbyt dÅ‚ugiego tekstu o zadanym pierwszym i ostatnim sÅ‚owie

Naturalna metoda czÄ™sto generuje zdania, gdzie ostatnie sÅ‚owo moÅ¼e nie pasowaÄ‡ do kontekstu, dlatego sprawdzenie i ponowne losowanie sÄ… nieefektywne.

**Proponowane rozwiÄ…zanie:**

	1.	**Beam search z optymalizacjÄ… na koÅ„cowe dopasowanie:** Rozpocznij generowanie od pierwszego sÅ‚owa, jednoczeÅ›nie prÃ³bujÄ…c zachowaÄ‡ moÅ¼liwy kontekst koÅ„cowego sÅ‚owa, uÅ¼ywajÄ…c beam search do zachowania kilku najbardziej prawdopodobnych wersji tekstu.
	2.	**Heurystyka ograniczenia dÅ‚ugoÅ›ci:** TworzÄ…c tekst, wprowadÅº heurystykÄ™, aby dÅ‚ugoÅ›Ä‡ byÅ‚a zgodna z zaÅ‚oÅ¼onym M, a rÃ³wnoczeÅ›nie koÅ„czyÅ‚a siÄ™ sÅ‚owem o wysokim prawdopodobieÅ„stwie w kontekÅ›cie ostatniego sÅ‚owa.
	3.	**Dwukierunkowe losowanie:** Rozpocznij generacjÄ™ zarÃ³wno od pierwszego sÅ‚owa w przÃ³d, jak i od ostatniego sÅ‚owa wstecz, Å‚Ä…czÄ…c te fragmenty tam, gdzie kontekst jest spÃ³jny.

### d) Losowanie tekstu o dÅ‚ugoÅ›ci n, w ktÃ³rym kaÅ¼de sÅ‚owo ma okreÅ›lony sufiks

Problem polega na tym, Å¼e naturalna metoda nie gwarantuje trafienia na sÅ‚owa o odpowiednim sufiksie w kolejnych losowaniach, co prowadzi do wielu nieudanych prÃ³b.

**Proponowane rozwiÄ…zanie:**

	1.	**Beam search skoncentrowany na dopasowaniu sufiksÃ³w:** Rozpocznij generowanie z moÅ¼liwoÅ›ciÄ… wyboru sÅ‚Ã³w z odpowiednimi sufiksami, stosujÄ…c beam search, aby kontynuowaÄ‡ tylko te Å›cieÅ¼ki, ktÃ³re majÄ… wysokie prawdopodobieÅ„stwo sukcesu.
	2.	**Budowanie sekwencyjne z filtracjÄ… sufiksÃ³w:** TworzÄ…c kolejne sÅ‚owa, filtruj sÅ‚owa o odpowiednich sufiksach, jednoczeÅ›nie minimalizujÄ…c liczbÄ™ moÅ¼liwych kontynuacji dla kaÅ¼dej Å›cieÅ¼ki.
	3.	*Zastosowanie algorytmu A do optymalizacji tekstu z dopasowaniem sufiksÃ³w:** Algorytm A* wykorzystuje heurystykÄ™, ktÃ³ra priorytetuje najbardziej prawdopodobne przejÅ›cia zgodne z wymaganiami, pozwalajÄ…c na szybsze znalezienie sekwencji o zadanych sufiksach.

## ==ğŸŸ¢Zadanie 4==
Jednym z przykÅ‚adÃ³w jest przykÅ‚adowo podpunkt a w poprzednim zadaniu. JeÅ¼eli nauczyliÅ›my bota ukÅ‚adaÄ‡ zdania od tyÅ‚u to jest w stanie zaczÄ…Ä‡ od zdania ktÃ³re na k-tym miejscu ma wyraz k-ty i uÅ‚oÅ¼yÄ‡ zdanie danej dÅ‚ugoÅ›ci. JesteÅ›my wiÄ™c w stanie ukÅ‚adaÄ‡ doÅ›Ä‡ dobrze zdania ktÃ³re majÄ… na jakimÅ› miejscu dany wyraz, bo najpierw uÅ¼yjemy odwrÃ³conego bota, a pÃ³Åºniej juÅ¼ normalnego (lewo-prawo).
 
### 3. Model moÅ¼e sÅ‚uÅ¼yÄ‡ jako fundament do transferu wiedzy

	* 	**Przyspieszenie treningu nowego modelu:** Z racji podobnej struktury i wielu wspÃ³lnych wzorcÃ³w jÄ™zykowych, wnioski wyciÄ…gniÄ™te z analizy wynikÃ³w tego modelu mogÄ… posÅ‚uÅ¼yÄ‡ jako â€podkÅ‚adâ€ do transfer learningu w niektÃ³rych specyficznych zadaniach lub przy optymalizacji parametrÃ³w treningu. W zwiÄ…zku z tym istnieje szansa na przyspieszenie treningu i poprawÄ™ jakoÅ›ci nowego modelu.
	* 	**Stworzenie sieci eksperymentalnej:** Model moÅ¼e posÅ‚uÅ¼yÄ‡ do badaÅ„ nad transferem wiedzy pomiÄ™dzy modelami przeszkolonymi na rÃ³Å¼nych wersjach danych, zwÅ‚aszcza w przypadku nietypowych ukÅ‚adÃ³w tekstu.

### 4. Model odwrÃ³conego jÄ™zyka moÅ¼e mieÄ‡ potencjalne zastosowania praktyczne

	* 	**Analiza struktury i odwrotnych fraz:** MoÅ¼na zbadaÄ‡ model, by lepiej zrozumieÄ‡, jak radzi sobie z odwrotnymi sekwencjami i czy daÅ‚oby siÄ™ go wykorzystaÄ‡ w zadaniach zwiÄ…zanych z odwracaniem tekstu lub generowaniem tekstu z nietypowÄ… strukturÄ….
	* 	**Aplikacje w przetwarzaniu zaszyfrowanych i znieksztaÅ‚conych danych:** Model moÅ¼e mieÄ‡ zastosowanie w obszarze analizy zaszyfrowanych tekstÃ³w, dekodowania nietypowych danych czy przetwarzania struktur o odwrotnej kolejnoÅ›ci, co mogÅ‚oby byÄ‡ cenne np. w szyfrowaniu lub kompresji tekstu.

## ==ğŸŸ¢Zadanie 7==
Dla wyrazÃ³w z wiÄ™kszym prawdopodobieÅ„stwem nadajemy krÃ³tsze ciÄ…gi bitowe, a dla wyrazÃ³w z wiÄ™kszym prawdopodobieÅ„stwem nadajemy dÅ‚uÅ¼sze ciÄ…gi bitowe. 

Model jÄ™zykowy dziaÅ‚a tak, Å¼e na podstawie poprzednich wyrazÃ³w daje prawdopodobieÅ„stwa wyrazÃ³w kolejnych. PoniewaÅ¼ zawsze prawdopodobieÅ„stwa sÄ… takie same moÅ¼emy uÅ¼yc to do kompresji. JeÅ›li znamy caÅ‚y poprzedni tekst oraz dostajemy jakiÅ› skompresowany token ktÃ³ry mÃ³wi nam ktÃ³re prawdopodobieÅ„stwo wybraÄ‡ moÅ¼emy odszyfrowaÄ‡ skompresowany tekst.
WiÄ™c kompresujemy go w nastÄ™pujÄ…cy sposÃ³b: 
IdÄ…c po kolei sÅ‚owami patrzymy jakie prawdopodobieÅ„stwa zostaÅ‚y przypisane odpowiednim wyrazom i na podstawie tego nadajemy jakiÅ› kod wyrazowi ktÃ³ry odpowiada temu ktÃ³ry powinien byÄ‡ kolejny w oryginalnym tekÅ›cie. **Ja bym to widziaÅ‚ tak, Å¼e moÅ¼emy przypisywaÄ‡ po prostu wartoÅ›Ä‡ liczbowÄ… rÃ³wnÄ… miejscu w posortowanej malejÄ…co tablicy prawdopodobieÅ„stw.** Wtedy nasz skompresowany tekst ma dla kaÅ¼dego wyrazu tylko jednÄ… liczbÄ™ zamiast ciÄ…gu znakÃ³w. MoÅ¼emy to kompresowaÄ‡ mocniej i np. uÅ¼ywaÄ‡ ciÄ…gu binarnego ktÃ³ry przypiszemy w jakiÅ› sposÃ³b.


## ==ğŸŸ¢Zadanie 8==

Problemem jest tokenizer, ktÃ³rego token moÅ¼e siÄ™ zaczynaÄ‡ od spacji. Takich tokenÃ³w bÄ™dzie o wiele mniej niÅ¼ tokenÃ³w niezaczynajÄ…cych siÄ™ od spacji przez to moÅ¼emy dostawaÄ‡ o wiele mniej prawdopodobne sÅ‚owa jako wynik.


## ==ğŸŸ¢Zadanie 9==
Ten tekst charakteryzuje siÄ™ duÅ¼Ä… iloÅ›ciÄ… rymÃ³w ktÃ³re powtarzajÄ… siÄ™ w jakiÅ› okreÅ›lonych schematach (np. na koÅ„cu linii ktÃ³ra moÅ¼e mieÄ‡ rÃ³Å¼nÄ… dÅ‚ugoÅ›Ä‡). Te rymy czasem siÄ™ rÃ³wnieÅ¼ zmieniajÄ… (nie chcemy ciÄ…gle rymowaÄ‡ tej samej koÅ„cÃ³wki wyrazu przez caÅ‚y tekst).
Ja bym to generowaÅ‚ w nastÄ™pujÄ…cy sposÃ³b:
1. Losujemy wyraz ktÃ³ry bÄ™dziemy rymowaÄ‡.  
2. Losujemy wyrazy i rymujemy na koÅ„cu kolejnej linii z wyrazem ktÃ³ry mieliÅ›my rymowaÄ‡.
3. Z jakimÅ› ppb zmieniamy rym i losujemy odpowiednio dla niego.


Taki tekst, jak podany wierszyk, charakteryzuje siÄ™ kilkoma cechami, ktÃ³re majÄ… wpÅ‚yw na generacjÄ™:

	1.	**Rym i rytm**: KaÅ¼da linijka lub co najmniej kaÅ¼da para linijek powinna rymowaÄ‡ siÄ™ i mieÄ‡ odpowiedni rytm, czyli liczbÄ™ sylab lub akcentÃ³w w linijce.
	2.	**PowtarzalnoÅ›Ä‡ i struktura**: W wierszykach czÄ™sto stosuje siÄ™ powtÃ³rzenia fraz lub struktur zdaÅ„ (np. â€W prawym bucie, w lewym bucieâ€), co nadaje tekstowi melodycznoÅ›Ä‡.
	3.	**Prosty jÄ™zyk i tematyka**: Wierszyki zwykle opisujÄ… proste sytuacje lub emocje w sposÃ³b obrazowy, co jest zrozumiaÅ‚e nawet dla dzieci.
	4.	**CzÄ™sto ograniczona liczba wyrazÃ³w**: WiÄ™kszoÅ›Ä‡ wierszykÃ³w jest krÃ³tka, wiÄ™c trzeba efektywnie wybieraÄ‡ wyrazy, aby zachowaÄ‡ treÅ›Ä‡ i rymy.

### Proponowany algorytm do generacji wierszykÃ³w z wykorzystaniem modelu (np. jak Papuga):

**Krok 1: OkreÅ›lenie struktury wiersza**

Zacznij od okreÅ›lenia liczby linijek, schematu rymÃ³w (np. ABAB lub AABB) oraz liczby sylab w kaÅ¼dej linijce. Na przykÅ‚ad:

	* 	Wierszyk bÄ™dzie miaÅ‚ 4 linijki.
	* 	Struktura rymÃ³w to AABB (linijka 1 rymuje siÄ™ z linijkÄ… 2, a linijka 3 z linijkÄ… 4).
	* 	KaÅ¼da linijka powinna mieÄ‡ np. 8 sylab.

**Krok 2: Wygenerowanie fraz rymujÄ…cych**

	1.	**WybÃ³r pierwszego wersu**: UÅ¼ywajÄ…c modelu, wygeneruj poczÄ…tkowy wers (np. â€Biega, krzyczy pan Hilaryâ€).
	2.	**Generowanie rymÃ³w**: Dla kaÅ¼dego nowego wersu potrzebujÄ…cego rymu (np. 2 i 4 wers), sprawdÅº, jakie wyrazy koÅ„cowe zostaÅ‚y wygenerowane dla poprzednich wersÃ³w i dopasuj rymy. MoÅ¼na to zrobiÄ‡ poprzez:
	* 	Przeszukiwanie bazy rymÃ³w lub sÅ‚ownika rymÃ³w, aby znaleÅºÄ‡ pasujÄ…ce wyrazy.
	* 	Wymuszanie modelu na generowanie okreÅ›lonego rymu przez iteracyjne prÃ³by, z uÅ¼yciem promptu podpowiadajÄ…cego.

**Krok 3: Zapewnienie rytmu (sylab i akcentÃ³w)**

	* 	Po wygenerowaniu kaÅ¼dej linijki przelicz liczbÄ™ sylab. JeÅ›li rytm jest nieodpowiedni, moÅ¼na lekko modyfikowaÄ‡ linijkÄ™ lub uÅ¼ywaÄ‡ narzÄ™dzi do wymiany sÅ‚Ã³w na synonimy, zachowujÄ…c odpowiedniÄ… liczbÄ™ sylab.
	* 	MoÅ¼na rÃ³wnieÅ¼ wymuszaÄ‡ na modelu generowanie prostych, rytmicznych konstrukcji przez ustawienie prostej struktury zdaÅ„ i konkretnego sÅ‚ownictwa.

**Krok 4: UÅ¼ycie beam search dla zwiÄ™kszenia spÃ³jnoÅ›ci**

	* 	GenerujÄ…c kolejne linijki, moÅ¼esz uÅ¼yÄ‡ beam search, aby wybraÄ‡ najlepsze frazy w kontekÅ›cie caÅ‚oÅ›ci wiersza. Beam search pozwala na rozwaÅ¼enie kilku alternatywnych fraz i wybÃ³r tej, ktÃ³ra najlepiej pasuje zarÃ³wno pod wzglÄ™dem treÅ›ci, jak i rymÃ³w.
	* 	Dla kaÅ¼dej linijki generuj kilka wersji i wybieraj te, ktÃ³re majÄ… najwiÄ™ksze prawdopodobieÅ„stwo przy zachowaniu sensu i rymÃ³w, by stworzyÄ‡ spÃ³jny wierszyk.

**Krok 5: Automatyczne dopasowanie struktury do wierszyka**

	* 	JeÅ›li model generuje wersy, ktÃ³re nie pasujÄ… do schematu rytmicznego lub rymowego, moÅ¼esz wprowadziÄ‡ dodatkowÄ… funkcjÄ™ korekty struktury: przeformuÅ‚owanie lub zamianÄ™ fragmentÃ³w wersu na pasujÄ…ce wyraÅ¼enia.
	* 	MoÅ¼esz takÅ¼e wymusiÄ‡ wprowadzenie konkretnych wyrazÃ³w lub fraz, by model powtarzaÅ‚ okreÅ›lone struktury (np. â€W prawym bucie, w lewym bucieâ€).

**Podsumowanie**

DziÄ™ki tej strategii moÅ¼na generowaÄ‡ wierszyki o poÅ¼Ä…danych wÅ‚aÅ›ciwoÅ›ciach â€“ z rymami, odpowiednim rytmem i spÃ³jnoÅ›ciÄ… treÅ›ci, minimalizujÄ…c potrzebÄ™ rÄ™cznych poprawek i maksymalizujÄ…c efektywnoÅ›Ä‡ generacji.